{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast\n",
    "import astunparse\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API 인증 토큰 (반드시 실제 토큰으로 변경)\n",
    "github_token = \"use your own token\"\n",
    "if github_token is None:\n",
    "    raise ValueError(\"GitHub API token not set. Please set the GITHUB_TOKEN environment variable.\")\n",
    "\n",
    "# CodeT5 모델 및 토크나이저 로드\n",
    "model_name = \"Salesforce/codet5-small\"                      # Salesforce/codet5-base 혹은 codet5-large 같은 더 큰 모델 사용 가능능\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 1. 데이터셋 로드\n",
    "################################################################\n",
    "# train 용 데이터셋 로드\n",
    "dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"dev\")\n",
    "# evaluation 용 데이터셋 로드드\n",
    "eval_dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 2. GitHub API 관련 함수\n",
    "################################################################\n",
    "# GitHub API를 사용하여 변경된 파일 목록 가져오는 함수\n",
    "def get_changed_files_api(repo_owner, repo_name, base_commit, github_token):\n",
    "    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/compare/{base_commit}^...{base_commit}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {github_token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        comparison = response.json()\n",
    "        changed_files = [file[\"filename\"] for file in comparison[\"files\"]]\n",
    "        return changed_files\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error getting changed files for commit {base_commit}: {e}\")\n",
    "        return None\n",
    "\n",
    "# GitHub API를 사용하여 변경된 파일들의 코드를 가져오는 함수\n",
    "def get_code_from_commit_api(repo_owner, repo_name, commit_hash, changed_files, github_token):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {github_token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3.raw\"\n",
    "    }\n",
    "    base_code_dict = {}\n",
    "    for file_path in changed_files:\n",
    "        api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}?ref={commit_hash}\"\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            base_code_dict[file_path] = response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error retrieving code for file {file_path} in commit {commit_hash}: {e}\")\n",
    "            return None\n",
    "    return base_code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 3. AST 기반 코드 파싱\n",
    "# 주석이나 불필요한 부분을 미리 제거하여 파싱 속도를 높일 수 있음\n",
    "################################################################\n",
    "# FunctionVisitor 클래스\n",
    "# 주어진 코드에서 동기/비동기 함수 정의를 찾아내어, 함수 이름, 코드, 시작/끝 라인, 그리고 파일 경로 등의 정보를 저장\n",
    "class FunctionVisitor(ast.NodeVisitor):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        self.functions = []\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.functions.append({\n",
    "            \"name\": node.name,\n",
    "            \"code\": astunparse.unparse(node),\n",
    "            \"start_line\": node.lineno,\n",
    "            \"end_line\": node.end_lineno,\n",
    "            \"filepath\": self.file_path\n",
    "        })\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_AsyncFunctionDef(self, node):\n",
    "        self.functions.append({\n",
    "            \"name\": node.name,\n",
    "            \"code\": astunparse.unparse(node),\n",
    "            \"start_line\": node.lineno,\n",
    "            \"end_line\": node.end_lineno,\n",
    "            \"filepath\": self.file_path\n",
    "        })\n",
    "        self.generic_visit(node)\n",
    "\n",
    "# AST 파싱 함수\n",
    "# 코드 텍스트를 파싱하여 AST를 생성하고, 위의 FunctionVisitor를 사용해 함수 단위로 코드를 분할\n",
    "# 파싱 오류가 발생하면 오류 메시지를 출력하고 빈 리스트를 반환\n",
    "def split_code_into_functions_ast(code_text, file_path):\n",
    "    try:\n",
    "        tree = ast.parse(code_text)\n",
    "        visitor = FunctionVisitor(file_path)\n",
    "        visitor.visit(tree)\n",
    "        return visitor.functions\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Syntax/Value error in code: {file_path}\\n{e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 4. 패딩 함수\n",
    "################################################################\n",
    "# 모델 입력 및 어텐션 마스크의 길이를 512로 맞추기 위해 기본 패딩 토큰과 0을 반환하는 함수\n",
    "def pad_int_list(pad_token_id=1, length=512):\n",
    "    return [pad_token_id]*length\n",
    "\n",
    "def pad_attention_list(length=512):\n",
    "    return [0]*length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 5. 데이터 전처리 함수\n",
    "################################################################\n",
    "# 데이터 전처리 함수 (BM25, AST 사용)\n",
    "'''\n",
    "1. 정보 추출: repo, base_commit, 문제 설명, 그리고 패치 정보를 추출\n",
    "2. GitHub API 호출: 변경된 파일 목록과 각 파일의 코드를 가져옴\n",
    "3. AST 파싱: 코드에서 함수 단위로 분할\n",
    "4. BM25 검색: 문제 설명과 가장 관련성 높은 함수 3개 선택 -> 상위 3개 함수 대신, 더 많은 후보를 선택한 후, 후처리나 re-ranking 단계를 거치는 방법도 고려\n",
    "5. Prompt 구성: \"Fix the bug:\"와 선택된 코드 정보를 포함하는 입력 텍스트 생성\n",
    "6. 토큰화 및 패딩: 입력 및 label(패치) 데이터를 512 토큰 길이로 맞춤\n",
    "'''\n",
    "def preprocess_function(batch):\n",
    "    out_input_ids = []\n",
    "    out_attention_mask = []\n",
    "    out_labels = []\n",
    "\n",
    "    for i in range(len(batch[\"repo\"])):\n",
    "        try:\n",
    "            repo = batch[\"repo\"][i]\n",
    "            base_commit = batch[\"base_commit\"][i]\n",
    "            problem_statement = batch[\"problem_statement\"][i]\n",
    "            patch = batch[\"patch\"][i]\n",
    "\n",
    "            repo_owner, repo_name = repo.split(\"/\")\n",
    "            changed_files = get_changed_files_api(repo_owner, repo_name, base_commit, github_token)\n",
    "            if not changed_files:\n",
    "                print(f\"No changed files found for commit {base_commit}. Skipping this example.\")\n",
    "                out_input_ids.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                out_attention_mask.append(pad_attention_list(512))\n",
    "                out_labels.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                continue\n",
    "\n",
    "            base_code_dict = get_code_from_commit_api(repo_owner, repo_name, base_commit, changed_files, github_token)\n",
    "            if base_code_dict is None:\n",
    "                print(f\"Failed to retrieve code for commit {base_commit}. Skipping this example.\")\n",
    "                out_input_ids.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                out_attention_mask.append(pad_attention_list(512))\n",
    "                out_labels.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                continue\n",
    "\n",
    "            all_functions = []\n",
    "            for file_path, base_code in base_code_dict.items():\n",
    "                functions = split_code_into_functions_ast(base_code, file_path)\n",
    "                all_functions.extend(functions)\n",
    "\n",
    "            if not all_functions:\n",
    "                print(f\"No functions found in commit {base_commit}. Skipping this example.\")\n",
    "                out_input_ids.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                out_attention_mask.append(pad_attention_list(512))\n",
    "                out_labels.append(pad_int_list(tokenizer.pad_token_id, 512))\n",
    "                continue\n",
    "\n",
    "            tokenized_functions = [tokenizer.tokenize(func[\"code\"]) for func in all_functions]\n",
    "            bm25 = BM25Okapi(tokenized_functions)\n",
    "            tokenized_query = tokenizer.tokenize(problem_statement)\n",
    "            doc_scores = bm25.get_scores(tokenized_query)\n",
    "            top_n = 3\n",
    "            top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_n]\n",
    "            selected_functions = [all_functions[i] for i in top_indices]\n",
    "\n",
    "            selected_code = \"\"\n",
    "            for func_info in selected_functions:\n",
    "                selected_code += (\n",
    "                    f\"File: {func_info['filepath']}, Function: {func_info['name']}\\n\"\n",
    "                    f\"Start Line: {func_info['start_line']}, End Line: {func_info['end_line']}\\n\"\n",
    "                    f\"Code:\\n{func_info['code']}\\n\\n\"\n",
    "                )\n",
    "    \n",
    "            input_text = f\"Fix the bug: {problem_statement}\\nCode:\\n{selected_code}\"\n",
    "            input_tokens = tokenizer.encode(input_text)\n",
    "            if len(input_tokens) > 512:\n",
    "                input_tokens = input_tokens[:512]\n",
    "            input_text = tokenizer.decode(input_tokens, skip_special_tokens=True)\n",
    "    \n",
    "            model_inputs = tokenizer(input_text, max_length=512, truncation=True)\n",
    "            labels = tokenizer(patch, max_length=512, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "            if \"input_ids\" not in model_inputs:\n",
    "                model_inputs[\"input_ids\"] = [tokenizer.pad_token_id] * 512\n",
    "            if \"attention_mask\" not in model_inputs:\n",
    "                model_inputs[\"attention_mask\"] = [0] * 512\n",
    "            if \"labels\" not in model_inputs:\n",
    "                model_inputs[\"labels\"] = [tokenizer.pad_token_id] * 512\n",
    "    \n",
    "            input_length = len(model_inputs[\"input_ids\"])\n",
    "            if input_length < 512:\n",
    "                model_inputs[\"input_ids\"].extend([tokenizer.pad_token_id] * (512 - input_length))\n",
    "                model_inputs[\"attention_mask\"].extend([0] * (512 - input_length))\n",
    "    \n",
    "            labels_length = len(model_inputs[\"labels\"])\n",
    "            if labels_length < 512:\n",
    "                model_inputs[\"labels\"].extend([tokenizer.pad_token_id] * (512 - labels_length))\n",
    "    \n",
    "            out_input_ids.append(model_inputs[\"input_ids\"])\n",
    "            out_attention_mask.append(model_inputs[\"attention_mask\"])\n",
    "            out_labels.append(model_inputs[\"labels\"])\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")\n",
    "            out_input_ids.append([])\n",
    "            out_attention_mask.append([])\n",
    "            out_labels.append([])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": out_input_ids,\n",
    "        \"attention_mask\": out_attention_mask,\n",
    "        \"labels\": out_labels\n",
    "    }\n",
    "\n",
    "# convert_to_features 함수\n",
    "# map 함수에서 호출하여 각 배치를 전처리 함수에 전달\n",
    "def convert_to_features(batch):\n",
    "    return preprocess_function(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 6. 데이터셋 전처리 및 필터링\n",
    "################################################################\n",
    "# 원본 데이터셋 전처리 및 필터링\n",
    "# 원본 데이터셋에 대해 전처리 함수를 적용한 후, 전처리 결과가 빈 값이 아닌 것만 남김\n",
    "# 전처리 후 사용되는 열은 \"input_ids\", \"attention_mask\", \"labels\"\n",
    "mapped_dataset = dataset.map(\n",
    "    convert_to_features,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    load_from_cache_file=False,\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "print(\"After map =>\", mapped_dataset.column_names)\n",
    "\n",
    "def nonempty_filter(example):\n",
    "    return len(example[\"input_ids\"]) > 0\n",
    "\n",
    "mapped_dataset = mapped_dataset.filter(nonempty_filter)\n",
    "print(\"After filter =>\", mapped_dataset.column_names)\n",
    "\n",
    "# 평가 데이터셋 전처리 및 필터링\n",
    "mapped_eval_dataset = eval_dataset.map(\n",
    "    convert_to_features,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "mapped_eval_dataset = mapped_eval_dataset.remove_columns(\n",
    "    [col for col in mapped_eval_dataset.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
    ")\n",
    "mapped_eval_dataset = mapped_eval_dataset.filter(nonempty_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 7. 모델 학습\n",
    "################################################################\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./codet5-finetuned-swe-bench-optimized\",\n",
    "    per_device_train_batch_size=2,  # 증가 가능\n",
    "    per_device_eval_batch_size=2,   # 증가 가능\n",
    "    gradient_accumulation_steps=2,  # 충분한 메모리로 accumulation 줄일 수 있음\n",
    "    learning_rate=3e-5,             # 실험에 따라 조정 (큰 배치 사이즈는 학습률이 높아져야 함)\n",
    "    num_train_epochs=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,       # 더 많은 워커 사용 가능\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_dataset,\n",
    "    eval_dataset=mapped_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "first_batch = next(iter(train_dataloader))\n",
    "print(\"First batch keys:\", first_batch.keys())\n",
    "print(\"First batch:\", first_batch)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 8. 추론(패치 생성)\n",
    "################################################################\n",
    "# 추론을 위한 준비 (파인튜닝된 모델 로드)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./codet5-finetuned-swe-bench-optimized\")\n",
    "model.to(device)\n",
    "\n",
    "# 추론 데이터셋 로드 및 패치 생성 함수\n",
    "'''\n",
    "1. 입력 정보 추출: repo, commit, 문제 설명을 가져옴\n",
    "2. GitHub API 호출: 변경된 파일과 코드를 가져옴\n",
    "3. AST & BM25: 함수 단위로 분할 후 문제 설명과 가장 관련있는 상위 3개 함수를 선택\n",
    "4. Prompt 구성: 선택된 함수 정보와 문제 설명을 포함하는 텍스트를 생성\n",
    "5. 모델 추론: 이 prompt를 토큰화하여 모델에 입력하고, generate() 함수로 패치를 생성\n",
    "'''\n",
    "output_dir = \"codet5_patches_bm25\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "test_dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "\n",
    "def generate_patch(example):\n",
    "    repo_owner = example[\"repo\"].split(\"/\")[0]\n",
    "    repo_name = example[\"repo\"].split(\"/\")[1]\n",
    "    commit_hash = example[\"base_commit\"]\n",
    "    problem_statement = example[\"problem_statement\"]\n",
    "\n",
    "    changed_files = get_changed_files_api(repo_owner, repo_name, commit_hash, github_token)\n",
    "    if not changed_files:\n",
    "        print(f\"No changed files found for commit {commit_hash}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    changed_files = [f for f in changed_files if f.endswith(\".py\")]\n",
    "    if not changed_files:\n",
    "        print(f\"No .py files found for commit {commit_hash}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    base_code_dict = get_code_from_commit_api(repo_owner, repo_name, commit_hash, changed_files, github_token)\n",
    "    if base_code_dict is None:\n",
    "        print(f\"Failed to retrieve code for commit {commit_hash}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    all_functions = []\n",
    "    for file_path, base_code in base_code_dict.items():\n",
    "        funcs = split_code_into_functions_ast(base_code, file_path)\n",
    "        all_functions.extend(funcs)\n",
    "    if not all_functions:\n",
    "        print(f\"No functions found in commit {commit_hash}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    tokenized_functions = [tokenizer.tokenize(func[\"code\"]) for func in all_functions]\n",
    "    bm25 = BM25Okapi(tokenized_functions)\n",
    "    tokenized_query = tokenizer.tokenize(problem_statement)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = 3\n",
    "    top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_n]\n",
    "    selected_functions = [all_functions[i] for i in top_indices]\n",
    "\n",
    "    selected_code = \"\"\n",
    "    for func_info in selected_functions:\n",
    "        selected_code += (\n",
    "            f\"File: {func_info['filepath']}, Function: {func_info['name']}\\n\"\n",
    "            f\"Start Line: {func_info['start_line']}, End Line: {func_info['end_line']}\\n\"\n",
    "            f\"Code:\\n{func_info['code']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt_input = (\n",
    "        f\"Issue: {problem_statement}\\n\"\n",
    "        f\"Code Context:\\n{selected_code}\\n\"\n",
    "        \"Task: Provide a patch to fix the issue.\"\n",
    "    )\n",
    "    \n",
    "    input_tokens = tokenizer.encode(prompt_input)\n",
    "    if len(input_tokens) > 512:\n",
    "        input_tokens = input_tokens[:512]\n",
    "    input_text = tokenizer.decode(input_tokens, skip_special_tokens=True)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=256, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    patch = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return patch\n",
    "\n",
    "# 추론 루프 및 파일 저장\n",
    "# 생성된 패치는 codet5_patches_bm25 폴더에 UTF-8 인코딩으로 저장\n",
    "for idx, example in enumerate(tqdm(test_dataset, desc=\"Generating Patches\")):\n",
    "    patch = generate_patch(example)\n",
    "    if patch:\n",
    "        with open(os.path.join(output_dir, f\"patch_{idx}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(patch)\n",
    "        print(f\"--- Example {idx} Patch ---\")\n",
    "        print(patch)\n",
    "    else:\n",
    "        print(f\"--- Example {idx} Skipped ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
