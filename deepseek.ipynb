{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -----------------------------\n",
    "# 1. GitHub API 통해 .py diff 가져오는 함수\n",
    "github_token = \"use your token\"  # 실제 토큰으로 변경\n",
    "if not github_token:\n",
    "    raise ValueError(\"GitHub API token not set.\")\n",
    "\n",
    "def get_py_diff_from_commit(repo, base_commit, github_token):\n",
    "    url = f\"https://api.github.com/repos/{repo}/compare/{base_commit}^...{base_commit}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {github_token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"GitHub API error: {response.status_code} {response.text}\")\n",
    "    \n",
    "    commit_data = response.json()\n",
    "    py_diffs = []\n",
    "    for file in commit_data.get(\"files\", []):\n",
    "        filename = file.get(\"filename\", \"\")\n",
    "        if filename.endswith(\".py\"):\n",
    "            patch = file.get(\"patch\")\n",
    "            if patch:\n",
    "                py_diffs.append(f\"File: {filename}\\n{patch}\\n\")\n",
    "    return \"\\n\".join(py_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 데이터셋 로드 (여기서는 princeton-nlp/SWE-bench_Verified)\n",
    "dataset = load_dataset(\"princeton-nlp/SWE-bench_Verified\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 프롬프트 생성 함수\n",
    "def build_prompt_with_diff(data, py_diff):\n",
    "    prompt = (\n",
    "        f\"A problem similar to '{data['problem_statement']}' has occurred, \"\n",
    "        \"and it seems that the root cause lies in the modified section of the .py file:\\n\\n\"\n",
    "    )\n",
    "    prompt += py_diff + \"\\n\\n\"\n",
    "    prompt += (\n",
    "        f\"{data.get('hints_text', '')}\\n\\n\"\n",
    "        \"Based on the information above, suggest a fix in the form of a code patch. \"\n",
    "        \"Do not include any unnecessary explanations—only provide the modified patch code. \"\n",
    "        \"Do not write any internal reasoning or thoughts. Only the modified patch code!\\n\"\n",
    "        \"### PATCH CODE START ###\\n\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. 모델 로드 (int8 양자화)\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# 아래는 예시로 device_map=\"cuda:0\"를 강제하는 모습\n",
    "custom_device_map = {\"\": \"cuda:0\"}\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,    # int8 양자화\n",
    "    device_map=custom_device_map\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. 모델로 수정 패치 코드 생성 함수\n",
    "def generate_response(prompt, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    delimiter = \"### PATCH CODE START ###\"\n",
    "    if delimiter in response:\n",
    "        response = response.split(delimiter, 1)[1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6. 전체 프로세스 실행\n",
    "all_patch_results = []  # JSON으로 저장할 결과를 담을 리스트\n",
    "output_dir = \"swe-bench-verified\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "json_output_path = os.path.join(output_dir, \"patch_results.jsonl\")\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"\\n--- Processing sample {i} ---\")\n",
    "    try:\n",
    "        repo = sample['repo']\n",
    "        base_commit = sample['base_commit']\n",
    "\n",
    "        py_diff = get_py_diff_from_commit(repo, base_commit, github_token)\n",
    "        print(\"Fetched .py diff:\")\n",
    "        print(py_diff)\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = build_prompt_with_diff(sample, py_diff)\n",
    "        print(\"Generated Prompt:\")\n",
    "        print(prompt)\n",
    "\n",
    "        # 모델 패치 생성\n",
    "        patch_code = generate_response(prompt, max_new_tokens=512)\n",
    "        print(\"Generated Patch Code:\")\n",
    "        print(patch_code)\n",
    "\n",
    "        # (A) JSON에 담을 형태로 구조화\n",
    "        result_dict = {\n",
    "            \"instance_id\": sample[\"instance_id\"],\n",
    "            \"model_patch\": patch_code,\n",
    "            \"model_name_or_path\": \"Changhyun Lee\"  # 사용자 지정\n",
    "        }\n",
    "\n",
    "        # 결과 리스트에 추가\n",
    "        all_patch_results.append(result_dict)\n",
    "\n",
    "        # (선택) 개별 텍스트 파일로도 저장\n",
    "        # output_file = os.path.join(output_dir, f\"patch_code_sample_{i}.txt\")\n",
    "        # with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        #     f.write(patch_code)\n",
    "        # print(f\"Patch code saved to {output_file}\")\n",
    "\n",
    "        # (B) 모든 루프 후, 하나의 JSON 파일에 저장\n",
    "        with open(json_output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            line = json.dumps(result_dict, ensure_ascii=False)\n",
    "            f.write(line + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {i}: {e}\")\n",
    "\n",
    "print(f\"\\nFinal JSONL results saved to: {json_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
